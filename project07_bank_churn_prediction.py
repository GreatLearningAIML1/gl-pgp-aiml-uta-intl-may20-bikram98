# -*- coding: utf-8 -*-
"""Project07_Bank Churn Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pvZrgnEnmpVl5xRBoO6QzvY9d6gexNkD

# Problem Statement

> ## Bank Churn Prediction 
> 
> ### Objective:
>Given a Bank customer, build a neural network based classifier that can determine whether they will leave
or not in the next 6 months. 
>
> ### Context: 
Businesses like banks which provide service have to worry about problem of 'Churn' i.e. customers
leaving and joining another service provider. It is important to understand which aspects of the service
influence a customer's decision in this regard. Management can concentrate efforts on improvement of
service, keeping in mind these priorities. 
>
> ### Data Description: 
The case study is from an open-source dataset from Kaggle.
The dataset contains 10,000 sample points with 14 distinct features such as Cust
Geography, Gender, Age, Tenure, Balance etc.
Link to the Kaggle project site:
https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling 
>
> ### Points Distribution: 
>The points distribution for this case is as follows: 
>
> 1. Read the dataset
> 2. Drop the columns which are unique for all users like IDs (5 points)
> 3. Distinguish the features and target variable (5 points)
> 4. Divide the data set into training and test sets (5 points)
> 5. Normalize the train and test data (10 points)
> 6. Initialize & build the model. Identify the points of improvement and implement the same the same.(20)
> 7. Predict the results using 0.5 as a threshold (10 points)
> 8. Print the Accuracy score and confusion matrix (5 points)

# **Bank Churn Prediction Project**
Submitted by: **Bikram M. Baruah**

### 1. Read the dataset
"""

#Loading the Essential Libraries
import pandas as pd
import numpy as np

# Mounting the appropriate drive containing data file
from google.colab import drive
drive.mount("/content/drive")

# reading the CSV file into pandas dataframe
df = pd.read_csv("/content/drive/My Drive/PYTHON/Churn_Modelling.csv")

"""## Basic EDA"""

# Printing the first  5 rows of the dataframe to have a look how the dataset look likes.
df.head()

#Checking the dimension
df.shape

#Checking the Dtype and if any null value exists
df.info()

#Checking the Statistics
df.describe().T

#Checking the value count of the suspected Target variable
df.Exited.value_counts()

#Making a pair plot to check interdependence of the variables.
import seaborn as sns
sns.pairplot(df)

"""Observations:
- There were no null values.
- The variables are mostly independent
- Some categorical features are there. They need to be one-hot-encoded.
- The continuous variables has wide range of values. They need to be normalized.

### 2. Drop the columns which are unique for all users like IDs (5 points)
- RowNumber and CustomerID are unique features and should be dropped for analysis.
- Surname is considered not essential for our analysis.
"""

# Dropping the non-essential columns
df2=df.drop(["RowNumber","CustomerId","Surname"], axis=1)

# Creating One-Hot-Encoding for the Categorical features
df2 = pd.get_dummies(df2,drop_first=True)

#Checking the final dataframe.
df2

"""# 3. Distinguish the features and target variable (5 points)
As the Objective of the problem statement is to determine if a customer will stay or leave the bank within next 6 months, the "Exited" column of the provided data indicates if the customer exited or not historically. Therefore, it is the Target variable.
"""

#Splitting the data into input features and target variable.
X = df2.drop(["Exited"],axis=1)
y = df2.Exited

"""# 4. Divide the data set into training and test sets (5 points)"""

# Importing `train_test_split` from `sklearn.model_selection`
from sklearn.model_selection import train_test_split

# Split the data up in train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=77)

# It also required to convert the target variable into Keras Categorical variable
from tensorflow.keras.utils import to_categorical
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

"""# 5. Normalize the train and test data (10 points)"""

from sklearn.preprocessing import StandardScaler

# Define the scaler 
scaler = StandardScaler().fit(X_train)

# Scale the train set
X_train = scaler.transform(X_train)

# Scale the test set
X_test = scaler.transform(X_test)

"""# 6. Initialize & build the model. Identify the points of improvement and implement the same.(20)

## 6.1 Model 1
"""

# Using Tensorflow Keras instead of the original Keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import optimizers

# Initialize the constructor
model_1 = Sequential()
model_1.add(Dense(12,activation='relu'))
model_1.add(Dense(2,activation='softmax'))

#Select Optimizer
opAdam = optimizers.Adam(0.01)

#Compile Model
model_1.compile(loss='categorical_crossentropy',optimizer=opAdam,metrics=['accuracy'])

#Train
model_1.fit(X_train, y_train_cat, epochs=50, verbose=1)

#Evaluation
loss, acc = model_1.evaluate(X_test, y_test_cat, verbose=1)
print("")
print("Evaluation:")
print('Accuracy: %.5f'  % acc)
print('Loss: %.5f' % loss)

"""## 6.2 Model 2"""

# Initialize the constructor
model_2 = Sequential()
model_2.add(Dense(12,activation='relu'))
model_2.add(Dense(2,activation='softmax'))

#Select Optimizer
opSGD = optimizers.SGD(learning_rate=0.01, momentum=0.1, nesterov=False, name='SGD')

#Compile Model
model_2.compile(loss='categorical_crossentropy',optimizer=opSGD,metrics=['accuracy'])

#Train
model_2.fit(X_train, y_train_cat, epochs=50, verbose=1)

#Evaluation
loss, acc = model_2.evaluate(X_test, y_test_cat, verbose=1)
print("")
print("Evaluation:")
print('Accuracy: %.5f'  % acc)
print('Loss: %.5f' % loss)

"""## 6.3 Model 3"""

# Initialize the constructor
model_3 = Sequential()
model_3.add(Dense(12,activation='relu'))
model_3.add(Dense(12,activation='sigmoid'))
model_3.add(Dense(12,activation='tanh'))
model_3.add(Dense(2,activation='softmax'))

#Select Optimizer
opSGD = optimizers.SGD(learning_rate=0.01, momentum=0.001, name='SGD')

#Compile Model
model_3.compile(loss='binary_crossentropy',optimizer=opSGD,metrics=['accuracy'])

#Train
model_3.fit(X_train, y_train_cat, epochs=100, verbose=1)

#Evaluation
loss, acc = model_3.evaluate(X_test, y_test_cat, verbose=1)
print("")
print("Evaluation:")
print('Accuracy: %.5f'  % acc)
print('Loss: %.5f' % loss)

"""## 6.4 Model 4"""

# Initialize the constructor
model_4 = Sequential()
model_4.add(Dense(12,activation='relu'))
model_4.add(Dense(60,activation='sigmoid'))
model_4.add(Dense(120,activation='tanh'))
model_4.add(Dense(120,activation='relu'))
model_4.add(Dense(60,activation='sigmoid'))
model_4.add(Dense(12,activation='tanh'))
model_4.add(Dense(2,activation='softmax'))

#Select Optimizer
opSGD = optimizers.SGD(learning_rate=0.01, momentum=0.001, name='SGD')

#Compile Model
model_4.compile(loss='binary_crossentropy',optimizer=opSGD,metrics=['accuracy'])

#Train
model_4.fit(X_train, y_train_cat, epochs=1000, verbose=1)

#Evaluation
loss, acc = model_4.evaluate(X_test, y_test_cat, verbose=1)
print("")
print("Evaluation:")
print('Accuracy: %.5f'  % acc)
print('Loss: %.5f' % loss)

"""## Comments
Several models were tested with different combinations of Layers (No. of Layers, No. of Nodes, Activation Function), Optimizers (Optimizer type, Learning Rate, Momentum) etc. Only four (4) were presented above. Their Off Sample (Test Data) Accuracy are summarized below:

Model 1: Accuracy= 0.86200

Model 2: Accuracy= 0.85633

Model 3: Accuracy= 0.86100

Model 4: Accuracy= 0.86067

Increasing the number of layers or nodes etc may improve the in-sample (training data) Accuracy, but it does not improve Off-Sample (Test Data) Accuracy beyond certain point. 

As, the best model is Model_1, it was selected for further analysis and detailed evaluation. This is model is simple and elegent and runs fast compared the other models.

## 7. Predict the results using 0.5 as a threshold (10 points)
"""

# model.predict() function will be used to predict if the customer will leave the bank in next 6 months or not. 
# The default threshold of this function is 0.5, it is not required to specify specifically.
model = model_1
y_predict = model.predict(X_test, verbose=1)

"""## 8. Print the Accuracy score and confusion matrix (5 points)"""

#Creating the Confusion Matrix 
from sklearn import metrics
y_pred = []
for val in y_predict:
    y_pred.append(np.argmax(val))
cm = metrics.confusion_matrix(y_test,y_pred)
print(cm)

#Classifictaion Report and Accuracy Score
cr=metrics.classification_report(y_test,y_pred,digits=5)
print(cr)

# Neural Network Summary
model.summary()

"""## Alternative Method of Prediction"""

y_pred_cls = model.predict_classes(X_test, batch_size=200, verbose=0)

cr=metrics.classification_report(y_test,y_pred_cls,digits=5)
print(cr)

"""# Final Comments
- Different combinations of Sequential Deep Neural Network attempted with varying hyperparameters.
- The best test accuracy that could be achieved was 0.86200.
- May be due to inherent randomness that cannot be explained with the given input parameters, accuracny could not be increased any further by adding additional hidden layers and nodes to the network.
-
"""

